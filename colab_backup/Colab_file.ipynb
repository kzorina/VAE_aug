{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Colab_file.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "_EbBUoBL39It",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "02b65e04-eb3c-4a95-98ff-1bbaa7058fb7",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1528573553976,
          "user_tz": -180,
          "elapsed": 2896,
          "user": {
            "displayName": "Kateryna Zorina",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "118398898453277774779"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from keras import backend as K\n",
        "from keras.engine import InputSpec, Layer\n",
        "from keras import initializers, regularizers, constraints\n",
        "import pickle"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "ndlTZBezOLjp",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "with open('caltech_dataset.pickle', 'rb') as data:\n",
        "    X_train, X_test, y_train, y_test = pickle.load(data)\n",
        "X_train = X_train.transpose(0, 3, 2, 1)\n",
        "X_test = X_test.transpose(0, 3, 2, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wyov0sBjTQso",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "c7e3f046-9a98-466e-c014-b2f3d509dd0b",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1528572980031,
          "user_tz": -180,
          "elapsed": 639,
          "user": {
            "displayName": "Kateryna Zorina",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "118398898453277774779"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#X_train = X_train.transpose(0, 3, 2, 1)\n",
        "##X_test = X_test.transpose(0, 3, 2, 1)\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(6941, 3, 150, 100)\n",
            "(1736, 3, 150, 100)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CFNIYfz7hY-7",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "class MinibatchDiscrimination(Layer):\n",
        "    \"\"\"Concatenates to each sample information about how different the input\n",
        "    features for that sample are from features of other samples in the same\n",
        "    minibatch, as described in Salimans et. al. (2016). Useful for preventing\n",
        "    GANs from collapsing to a single output. When using this layer, generated\n",
        "    samples and reference samples should be in separate batches.\n",
        "    # Example\n",
        "    ```python\n",
        "        # apply a convolution 1d of length 3 to a sequence with 10 timesteps,\n",
        "        # with 64 output filters\n",
        "        model = Sequential()\n",
        "        model.add(Convolution1D(64, 3, border_mode='same', input_shape=(10, 32)))\n",
        "        # now model.output_shape == (None, 10, 64)\n",
        "        # flatten the output so it can be fed into a minibatch discrimination layer\n",
        "        model.add(Flatten())\n",
        "        # now model.output_shape == (None, 640)\n",
        "        # add the minibatch discrimination layer\n",
        "        model.add(MinibatchDiscrimination(5, 3))\n",
        "        # now model.output_shape = (None, 645)\n",
        "    ```\n",
        "    # Arguments\n",
        "        nb_kernels: Number of discrimination kernels to use\n",
        "            (dimensionality concatenated to output).\n",
        "        kernel_dim: The dimensionality of the space where closeness of samples\n",
        "            is calculated.\n",
        "        init: name of initialization function for the weights of the layer\n",
        "            (see [initializations](../initializations.md)),\n",
        "            or alternatively, Theano function to use for weights initialization.\n",
        "            This parameter is only relevant if you don't pass a `weights` argument.\n",
        "        weights: list of numpy arrays to set as initial weights.\n",
        "        W_regularizer: instance of [WeightRegularizer](../regularizers.md)\n",
        "            (eg. L1 or L2 regularization), applied to the main weights matrix.\n",
        "        activity_regularizer: instance of [ActivityRegularizer](../regularizers.md),\n",
        "            applied to the network output.\n",
        "        W_constraint: instance of the [constraints](../constraints.md) module\n",
        "            (eg. maxnorm, nonneg), applied to the main weights matrix.\n",
        "        input_dim: Number of channels/dimensions in the input.\n",
        "            Either this argument or the keyword argument `input_shape`must be\n",
        "            provided when using this layer as the first layer in a model.\n",
        "    # Input shape\n",
        "        2D tensor with shape: `(samples, input_dim)`.\n",
        "    # Output shape\n",
        "        2D tensor with shape: `(samples, input_dim + nb_kernels)`.\n",
        "    # References\n",
        "        - [Improved Techniques for Training GANs](https://arxiv.org/abs/1606.03498)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, nb_kernels, kernel_dim, init='glorot_uniform', weights=None,\n",
        "                 W_regularizer=None, activity_regularizer=None,\n",
        "                 W_constraint=None, input_dim=None, **kwargs):\n",
        "        self.init = initializers.get(init)\n",
        "        self.nb_kernels = nb_kernels\n",
        "        self.kernel_dim = kernel_dim\n",
        "        self.input_dim = input_dim\n",
        "\n",
        "        self.W_regularizer = regularizers.get(W_regularizer)\n",
        "        self.activity_regularizer = regularizers.get(activity_regularizer)\n",
        "\n",
        "        self.W_constraint = constraints.get(W_constraint)\n",
        "\n",
        "        self.initial_weights = weights\n",
        "        self.input_spec = [InputSpec(ndim=2)]\n",
        "\n",
        "        if self.input_dim:\n",
        "            kwargs['input_shape'] = (self.input_dim,)\n",
        "        super(MinibatchDiscrimination, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) == 2\n",
        "\n",
        "        input_dim = input_shape[1]\n",
        "        self.input_spec = [InputSpec(dtype=K.floatx(),\n",
        "                                     shape=(None, input_dim))]\n",
        "\n",
        "        self.W = self.add_weight(shape=(self.nb_kernels, input_dim, self.kernel_dim),\n",
        "            initializer=self.init,\n",
        "            name='kernel',\n",
        "            regularizer=self.W_regularizer,\n",
        "            trainable=True,\n",
        "            constraint=self.W_constraint)\n",
        "\n",
        "        # Set built to true.\n",
        "        super(MinibatchDiscrimination, self).build(input_shape)\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        activation = K.reshape(K.dot(x, self.W), (-1, self.nb_kernels, self.kernel_dim))\n",
        "        diffs = K.expand_dims(activation, 3) - K.expand_dims(K.permute_dimensions(activation, [1, 2, 0]), 0)\n",
        "        abs_diffs = K.sum(K.abs(diffs), axis=2)\n",
        "        minibatch_features = K.sum(K.exp(-abs_diffs), axis=2)\n",
        "        return K.concatenate([x, minibatch_features], 1)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        assert input_shape and len(input_shape) == 2\n",
        "        return input_shape[0], input_shape[1]+self.nb_kernels\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {'nb_kernels': self.nb_kernels,\n",
        "                  'kernel_dim': self.kernel_dim,\n",
        "                  'init': self.init.__name__,\n",
        "                  'W_regularizer': self.W_regularizer.get_config() if self.W_regularizer else None,\n",
        "                  'activity_regularizer': self.activity_regularizer.get_config() if self.activity_regularizer else None,\n",
        "                  'W_constraint': self.W_constraint.get_config() if self.W_constraint else None,\n",
        "                  'input_dim': self.input_dim}\n",
        "        base_config = super(MinibatchDiscrimination, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4FoMdls2hV85",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 1492
        },
        "outputId": "59f3bf96-dd0f-4ffe-b6bb-dda2377fbab7",
        "executionInfo": {
          "status": "error",
          "timestamp": 1528575351282,
          "user_tz": -180,
          "elapsed": 1792495,
          "user": {
            "displayName": "Kateryna Zorina",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "118398898453277774779"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "file: ACGAN - CIFAR10.py\n",
        "author: Luke\n",
        "de\n",
        "Oliveira(lukedeo @ vaitech.io)\n",
        "contributor: KnightTuYa(398225157 @ qq.com)\n",
        "Consult\n",
        "https: // github.com / lukedeo / keras - acgan\n",
        "for MNIST version!\n",
        "Consult\n",
        "https: // github.com / soumith / ganhacks\n",
        "for GAN trick!\n",
        "I directly use Minibatch Layer Code from:\n",
        "https://github.com/forcecore/Keras-GAN-Animeface-Character\n",
        "Thanks for the great work!\n",
        "I am still not satisfied with the generated images yet, Any suggestion is welcomed!\n",
        "\"\"\"\n",
        "\n",
        "with open('caltech_dataset.pickle', 'rb') as data:\n",
        "    X_train, X_test, y_train, y_test = pickle.load(data)\n",
        "X_train = X_train.transpose(0, 3, 1, 2)\n",
        "X_test = X_test.transpose(0, 3, 1, 2)\n",
        "\n",
        "\n",
        "from __future__ import print_function\n",
        "import os\n",
        "from collections import defaultdict\n",
        "\n",
        "try:\n",
        "    import cPickle as pickle\n",
        "except ImportError:\n",
        "    import pickle\n",
        "from PIL import Image\n",
        "from six.moves import range\n",
        "import keras.backend as K\n",
        "from keras.datasets import cifar10, mnist\n",
        "from keras import layers\n",
        "from keras.layers import Input, Dense, Reshape, Flatten, Embedding, Dropout, BatchNormalization\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers.convolutional import Conv2DTranspose, Conv2D\n",
        "from keras.models import Sequential, Model\n",
        "from keras.optimizers import Adam\n",
        "from keras.initializers import TruncatedNormal\n",
        "from keras.utils.generic_utils import Progbar\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.layers.noise import GaussianNoise\n",
        "import numpy as np\n",
        "\n",
        "np.random.seed(1337)\n",
        "class_num = 101\n",
        "K.set_image_dim_ordering('th')\n",
        "path = \"images\"  # The path to store the generated images\n",
        "load_weight = False\n",
        "# Set True if you need to reload weight\n",
        "load_epoch = 73  # Decide which epoch to reload weight, please check your file name\n",
        "\n",
        "def build_generator(latent_size):\n",
        "    # we will map a pair of (z, L), where z is a latent vector and L is a\n",
        "    # label drawn from P_c, to image space (..., 3, 32, 32)\n",
        "    cnn = Sequential()\n",
        "    cnn.add(Dense(384 * 2 * 3, input_dim=latent_size, activation='relu',\n",
        "                  kernel_initializer='glorot_normal', bias_initializer='Zeros'))\n",
        "    cnn.add(Reshape((384, 2, 3)))\n",
        "\n",
        "    cnn.add(Conv2DTranspose(192, kernel_size=5, strides=5, padding='same', activation='relu',\n",
        "                            kernel_initializer='glorot_normal', bias_initializer='Zeros'))\n",
        "    cnn.add(BatchNormalization())\n",
        "\n",
        "    cnn.add(Conv2DTranspose(96, kernel_size=5, strides=5, padding='same', activation='relu',\n",
        "                            kernel_initializer='glorot_normal', bias_initializer='Zeros'))\n",
        "    cnn.add(BatchNormalization())\n",
        "\n",
        "    cnn.add(Conv2DTranspose(3, kernel_size=5, strides=2, padding='same', activation='tanh',\n",
        "                            kernel_initializer='glorot_normal', bias_initializer='Zeros'))\n",
        "    cnn.summary()\n",
        "    # this is the z space commonly refered to in GAN papers\n",
        "    latent = Input(shape=(latent_size,))\n",
        "\n",
        "    # this will be our label\n",
        "    image_class = Input(shape=(1,), dtype='int32')\n",
        "\n",
        "    # 10 classes in CIFAR-10\n",
        "    cls = Flatten()(Embedding(101, latent_size,\n",
        "                              embeddings_initializer='glorot_normal')(image_class))\n",
        "\n",
        "    # hadamard product between z-space and a class conditional embedding\n",
        "    h = layers.multiply([latent, cls])\n",
        "\n",
        "    fake_image = cnn(h)\n",
        "\n",
        "    return Model([latent, image_class], fake_image)\n",
        "\n",
        "\n",
        "def build_discriminator():\n",
        "    # build a relatively standard conv net, with LeakyReLUs as suggested in\n",
        "    # the reference paper\n",
        "    cnn = Sequential()\n",
        "\n",
        "    cnn.add(GaussianNoise(0.05, input_shape=(3, 100, 150)))  # Add this layer to prevent D from overfitting!\n",
        "\n",
        "    cnn.add(Conv2D(16, kernel_size=3, strides=2, padding='same',\n",
        "                   kernel_initializer='glorot_normal', bias_initializer='Zeros'))\n",
        "    cnn.add(LeakyReLU(alpha=0.2))\n",
        "    cnn.add(Dropout(0.5))\n",
        "\n",
        "    cnn.add(Conv2D(32, kernel_size=3, strides=1, padding='same',\n",
        "                   kernel_initializer='glorot_normal', bias_initializer='Zeros'))\n",
        "    cnn.add(BatchNormalization())\n",
        "    cnn.add(LeakyReLU(alpha=0.2))\n",
        "    cnn.add(Dropout(0.5))\n",
        "\n",
        "    cnn.add(Conv2D(64, kernel_size=3, strides=2, padding='same',\n",
        "                   kernel_initializer='glorot_normal', bias_initializer='Zeros'))\n",
        "    cnn.add(BatchNormalization())\n",
        "    cnn.add(LeakyReLU(alpha=0.2))\n",
        "    cnn.add(Dropout(0.5))\n",
        "\n",
        "    cnn.add(Conv2D(128, kernel_size=3, strides=1, padding='same',\n",
        "                   kernel_initializer='glorot_normal', bias_initializer='Zeros'))\n",
        "    cnn.add(BatchNormalization())\n",
        "    cnn.add(LeakyReLU(alpha=0.2))\n",
        "    cnn.add(Dropout(0.5))\n",
        "\n",
        "    cnn.add(Conv2D(256, kernel_size=3, strides=2, padding='same',\n",
        "                   kernel_initializer='glorot_normal', bias_initializer='Zeros'))\n",
        "    cnn.add(BatchNormalization())\n",
        "    cnn.add(LeakyReLU(alpha=0.2))\n",
        "    cnn.add(Dropout(0.5))\n",
        "\n",
        "    cnn.add(Conv2D(512, kernel_size=3, strides=1, padding='same',\n",
        "                   kernel_initializer='glorot_normal', bias_initializer='Zeros'))\n",
        "    cnn.add(BatchNormalization())\n",
        "    cnn.add(LeakyReLU(alpha=0.2))\n",
        "    cnn.add(Dropout(0.5))\n",
        "\n",
        "    cnn.add(Flatten())\n",
        "\n",
        "    cnn.add(MinibatchDiscrimination(50, 30))\n",
        "\n",
        "    image = Input(shape=(3, 100, 150))\n",
        "\n",
        "    features = cnn(image)\n",
        "\n",
        "    # first output (name=generation) is whether or not the discriminator\n",
        "    # thinks the image that is being shown is fake, and the second output\n",
        "    # (name=auxiliary) is the class that the discriminator thinks the image\n",
        "    # belongs to.\n",
        "    fake = Dense(1, activation='sigmoid', name='generation',\n",
        "                 kernel_initializer='glorot_normal', bias_initializer='Zeros')(features)\n",
        "    aux = Dense(class_num, activation='softmax', name='auxiliary',\n",
        "                kernel_initializer='glorot_normal', bias_initializer='Zeros')(features)\n",
        "\n",
        "    return Model(image, [fake, aux])\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    # batch and latent size taken from the paper\n",
        "    nb_epochs = 100\n",
        "    batch_size = 100\n",
        "    latent_size = 210\n",
        "\n",
        "    # Adam parameters suggested in https://arxiv.org/abs/1511.06434\n",
        "    adam_lr = 0.0002\n",
        "    adam_beta_1 = 0.5\n",
        "\n",
        "    # build the discriminator, Choose Adam as optimizer according to GANHACK\n",
        "    discriminator = build_discriminator()\n",
        "    discriminator.compile(\n",
        "        optimizer=Adam(lr=adam_lr, beta_1=adam_beta_1),\n",
        "        loss=['binary_crossentropy', 'sparse_categorical_crossentropy']\n",
        "    )\n",
        "    generator = build_generator(latent_size)\n",
        "\n",
        "    latent = Input(shape=(latent_size,))\n",
        "    image_class = Input(shape=(1,), dtype='int32')\n",
        "\n",
        "    # get a fake image\n",
        "    fake = generator([latent, image_class])\n",
        "    print(fake.shape)\n",
        "    # we only want to be able to train generator for the combined model\n",
        "    discriminator.trainable = False\n",
        "    fake, aux = discriminator(fake)\n",
        "    combined = Model([latent, image_class], [fake, aux])\n",
        "\n",
        "    combined.compile(\n",
        "        optimizer=Adam(lr=adam_lr, beta_1=adam_beta_1),\n",
        "        loss=['binary_crossentropy', 'sparse_categorical_crossentropy']\n",
        "    )\n",
        "\n",
        "    \n",
        "    X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
        "    X_test = (X_test.astype(np.float32) - 127.5) / 127.5\n",
        "    nb_train, nb_test = X_train.shape[0], X_test.shape[0]\n",
        "\n",
        "    train_history = defaultdict(list)\n",
        "    test_history = defaultdict(list)\n",
        "\n",
        "    if load_weight:\n",
        "        generator.load_weights('params_generator_epoch_{0:03d}.hdf5'.format(load_epoch))\n",
        "        discriminator.load_weights('params_discriminator_epoch_{0:03d}.hdf5'.format(load_epoch))\n",
        "    else:\n",
        "        load_epoch = 0\n",
        "\n",
        "    for epoch in range(nb_epochs):\n",
        "        print('Epoch {} of {}'.format(load_epoch + 1, nb_epochs))\n",
        "        load_epoch += 1\n",
        "        nb_batches = int(X_train.shape[0] / batch_size)\n",
        "        progress_bar = Progbar(target=nb_batches)\n",
        "\n",
        "        epoch_gen_loss = []\n",
        "        epoch_disc_loss = []\n",
        "\n",
        "        for index in range(nb_batches):\n",
        "            progress_bar.update(index)\n",
        "            # generate a new batch of noise\n",
        "            noise = np.random.normal(0, 0.5, (batch_size, latent_size))\n",
        "\n",
        "            # get a batch of real images\n",
        "            image_batch = X_train[index * batch_size:(index + 1) * batch_size]\n",
        "            label_batch = y_train[index * batch_size:(index + 1) * batch_size]\n",
        "\n",
        "            # sample some labels from p_c\n",
        "            sampled_labels = np.random.randint(0, class_num, batch_size)\n",
        "\n",
        "            # generate a batch of fake images, using the generated labels as a\n",
        "            # conditioner. We reshape the sampled labels to be\n",
        "            # (batch_size, 1) so that we can feed them into the embedding\n",
        "            # layer as a length one sequence\n",
        "            generated_images = generator.predict(\n",
        "                [noise, sampled_labels.reshape((-1, 1))], verbose=0)\n",
        "\n",
        "            disc_real_weight = [np.ones(batch_size), 2 * np.ones(batch_size)]\n",
        "            disc_fake_weight = [np.ones(batch_size), np.zeros(batch_size)]\n",
        "\n",
        "            # According to GANHACK, We training our ACGAN-CIFAR10 in Real->D, Fake->D,\n",
        "            # Noise->G, rather than traditional method: [Real, Fake]->D, Noise->G, actully,\n",
        "            # it really make sense!\n",
        "\n",
        "            for train_ix in range(3):\n",
        "                if index % 30 != 0:\n",
        "                    X_real = image_batch\n",
        "                    # Label Soomthing\n",
        "                    y_real = np.random.uniform(0.7, 1.2, size=(batch_size,))\n",
        "                    aux_y1 = label_batch.reshape(-1, )\n",
        "                    epoch_disc_loss.append(discriminator.train_on_batch(X_real, [y_real, aux_y1]))\n",
        "                    # Label Soomthing\n",
        "                    X_fake = generated_images\n",
        "                    y_fake = np.random.uniform(0.0, 0.3, size=(batch_size,))\n",
        "                    aux_y2 = sampled_labels\n",
        "\n",
        "                    # see if the discriminator can figure itself out...\n",
        "                    epoch_disc_loss.append(discriminator.train_on_batch(X_fake, [y_fake, aux_y2]))\n",
        "                else:\n",
        "                    # make the labels the noisy for the discriminator: occasionally flip the labels\n",
        "                    # when training the discriminator\n",
        "                    X_real = image_batch\n",
        "                    y_real = np.random.uniform(0.0, 0.3, size=(batch_size,))\n",
        "                    aux_y1 = label_batch.reshape(-1, )\n",
        "\n",
        "                    epoch_disc_loss.append(discriminator.train_on_batch(X_real, [y_real, aux_y1]))\n",
        "                    # Label Soomthing\n",
        "                    X_fake = generated_images\n",
        "                    y_fake = np.random.uniform(0.7, 1.2, size=(batch_size,))\n",
        "                    aux_y2 = sampled_labels\n",
        "\n",
        "                    # see if the discriminator can figure itself out...\n",
        "                    epoch_disc_loss.append(discriminator.train_on_batch(X_fake, [y_fake, aux_y2]))\n",
        "            # make new noise. we generate Guassian Noise rather than Uniform Noise according to GANHACK\n",
        "            noise = np.random.normal(0, 0.5, (2 * batch_size, latent_size))\n",
        "            sampled_labels = np.random.randint(0, class_num, 2 * batch_size)\n",
        "\n",
        "            # we want to train the generator to trick the discriminator\n",
        "            # For the generator, we want all the {fake, not-fake} labels to say\n",
        "            # not-fake\n",
        "            trick = np.random.uniform(0.7, 1.2, size=(2 * batch_size,))\n",
        "\n",
        "            epoch_gen_loss.append(combined.train_on_batch(\n",
        "                [noise, sampled_labels.reshape((-1, 1))], [trick, sampled_labels]))\n",
        "\n",
        "        print('\\nTesting for epoch {}:'.format(load_epoch))\n",
        "\n",
        "        # evaluate the testing loss here\n",
        "\n",
        "        # generate a new batch of noise\n",
        "        noise = np.random.normal(0, 0.5, (nb_test, latent_size))\n",
        "\n",
        "        # sample some labels from p_c and generate images from them\n",
        "        sampled_labels = np.random.randint(0, class_num, nb_test)\n",
        "        generated_images = generator.predict(\n",
        "            [noise, sampled_labels.reshape((-1, 1))], verbose=False)\n",
        "\n",
        "        X = np.concatenate((X_test, generated_images))\n",
        "        y = np.array([1] * nb_test + [0] * nb_test)\n",
        "        aux_y = np.concatenate((y_test.reshape(-1, ), sampled_labels), axis=0)\n",
        "\n",
        "        # see if the discriminator can figure itself out...\n",
        "        discriminator_test_loss = discriminator.evaluate(\n",
        "            X, [y, aux_y], verbose=False)\n",
        "\n",
        "        discriminator_train_loss = np.mean(np.array(epoch_disc_loss), axis=0)\n",
        "\n",
        "        # make new noise\n",
        "        noise = np.random.normal(0, 0.5, (2 * nb_test, latent_size))\n",
        "        sampled_labels = np.random.randint(0, class_num, 2 * nb_test)\n",
        "        trick = np.ones(2 * nb_test)\n",
        "        generator_test_loss = combined.evaluate(\n",
        "            [noise, sampled_labels.reshape((-1, 1))],\n",
        "            [trick, sampled_labels], verbose=False)\n",
        "\n",
        "        generator_train_loss = np.mean(np.array(epoch_gen_loss), axis=0)\n",
        "\n",
        "        # generate an epoch report on performance\n",
        "        train_history['generator'].append(generator_train_loss)\n",
        "        train_history['discriminator'].append(discriminator_train_loss)\n",
        "\n",
        "        test_history['generator'].append(generator_test_loss)\n",
        "        test_history['discriminator'].append(discriminator_test_loss)\n",
        "\n",
        "        print('{0:<22s} | {1:4s} | {2:15s} | {3:5s}'.format(\n",
        "            'component', *discriminator.metrics_names))\n",
        "        print('-' * 65)\n",
        "\n",
        "        ROW_FMT = '{0:<22s} | {1:<4.2f} | {2:<15.2f} | {3:<5.2f}'\n",
        "        print(ROW_FMT.format('generator (train)',\n",
        "                             *train_history['generator'][-1]))\n",
        "        print(ROW_FMT.format('generator (test)',\n",
        "                             *test_history['generator'][-1]))\n",
        "        print(ROW_FMT.format('discriminator (train)',\n",
        "                             *train_history['discriminator'][-1]))\n",
        "        print(ROW_FMT.format('discriminator (test)',\n",
        "                             *test_history['discriminator'][-1]))\n",
        "\n",
        "        # save weights every epoch\n",
        "        generator.save_weights(\n",
        "            'params_generator_epoch_{0:03d}.hdf5'.format(load_epoch), True)\n",
        "        discriminator.save_weights(\n",
        "            'params_discriminator_epoch_{0:03d}.hdf5'.format(load_epoch), True)\n",
        "\n",
        "        # generate some pictures to display\n",
        "        noise = np.random.normal(0, 0.5, (10*101, latent_size))\n",
        "        sampled_labels = np.array([\n",
        "            [i] * 10 for i in range(101)\n",
        "        ]).reshape(-1, 1)\n",
        "        generated_images = generator.predict([noise, sampled_labels])#.transpose(0, 2, 3, 1)\n",
        "        print(generated_images.shape)\n",
        "        generated_images = np.asarray((generated_images * 127.5 + 127.5).astype(np.uint8))\n",
        "\n",
        "\n",
        "#         def vis_square(data, padsize=1, padval=0):\n",
        "\n",
        "#             # force the number of filters to be square\n",
        "#             n = int(np.ceil(np.sqrt(data.shape[0])))\n",
        "#             padding = ((0, n ** 2 - data.shape[0]), (0, padsize), (0, padsize)) + ((0, 0),) * (data.ndim - 3)\n",
        "#             data = np.pad(data, padding, mode='constant', constant_values=(padval, padval))\n",
        "\n",
        "#             # tile the filters into an image\n",
        "#             data = data.reshape((n, n) + data.shape[1:]).transpose((0, 2, 1, 3) + tuple(range(4, data.ndim + 1)))\n",
        "#             data = data.reshape((n * data.shape[1], n * data.shape[3]) + data.shape[4:])\n",
        "#             return data\n",
        "\n",
        "\n",
        "#         img = vis_square(generated_images)\n",
        "#         if not os.path.exists(path):\n",
        "#             os.makedirs(path)\n",
        "#         Image.fromarray(img).save(\n",
        "#             'images/plot_epoch_{0:03d}_generated.png'.format(load_epoch))\n",
        "        \n",
        "        \n",
        "        if load_epoch % 5 == 0:\n",
        "#             image_path = 'images/plot_epoch_{0:03d}_generated.png'.format(load_epoch)\n",
        "            gen_path = 'params_generator_epoch_{0:03d}.hdf5'.format(load_epoch)\n",
        "            dis_path = 'params_discriminator_epoch_{0:03d}.hdf5'.format(load_epoch)\n",
        "#             files.download(image_path)\n",
        "            files.download(gen_path)\n",
        "            files.download(dis_path)\n",
        "        \n",
        "        pickle.dump({'train': train_history, 'test': test_history},\n",
        "                        open('acgan-history.pkl', 'wb'))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 2304)              486144    \n",
            "_________________________________________________________________\n",
            "reshape_1 (Reshape)          (None, 384, 2, 3)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_1 (Conv2DTr (None, 192, 10, 15)       1843392   \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 192, 10, 15)       60        \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_2 (Conv2DTr (None, 96, 50, 75)        460896    \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 96, 50, 75)        300       \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_3 (Conv2DTr (None, 3, 100, 150)       7203      \n",
            "=================================================================\n",
            "Total params: 2,797,995\n",
            "Trainable params: 2,797,815\n",
            "Non-trainable params: 180\n",
            "_________________________________________________________________\n",
            "(?, 3, ?, ?)\n",
            "Epoch 1 of 100\n",
            " 0/69 [..............................] - ETA: 0s"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:975: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "  'Discrepancy between trainable weights and collected trainable'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "68/69 [============================>.] - ETA: 4s\n",
            "Testing for epoch 1:\n",
            "component              | loss | generation_loss | auxiliary_loss\n",
            "-----------------------------------------------------------------\n",
            "generator (train)      | 14.40 | 2.71            | 11.70\n",
            "generator (test)       | 13.20 | 0.00            | 13.20\n",
            "discriminator (train)  | 9.49 | 0.85            | 8.64 \n",
            "discriminator (test)   | 17.49 | 8.04            | 9.45 \n",
            "(1010, 3, 100, 150)\n",
            "Epoch 2 of 100\n",
            "68/69 [============================>.] - ETA: 4s\n",
            "Testing for epoch 2:\n",
            "component              | loss | generation_loss | auxiliary_loss\n",
            "-----------------------------------------------------------------\n",
            "generator (train)      | 7.03 | 1.69            | 5.34 \n",
            "generator (test)       | 10.99 | 0.00            | 10.99\n",
            "discriminator (train)  | 4.48 | 0.43            | 4.04 \n",
            "discriminator (test)   | 13.80 | 5.69            | 8.10 \n",
            "(1010, 3, 100, 150)\n",
            "Epoch 3 of 100\n",
            "59/69 [========================>.....] - ETA: 47s"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "68/69 [============================>.] - ETA: 4s\n",
            "Testing for epoch 3:\n",
            "component              | loss | generation_loss | auxiliary_loss\n",
            "-----------------------------------------------------------------\n",
            "generator (train)      | 6.78 | 1.69            | 5.10 \n",
            "generator (test)       | 5.12 | 0.16            | 4.96 \n",
            "discriminator (train)  | 4.18 | 0.44            | 3.74 \n",
            "discriminator (test)   | 5.65 | 1.16            | 4.49 \n",
            "(1010, 3, 100, 150)\n",
            "Epoch 4 of 100\n",
            "68/69 [============================>.] - ETA: 4s\n",
            "Testing for epoch 4:\n",
            "component              | loss | generation_loss | auxiliary_loss\n",
            "-----------------------------------------------------------------\n",
            "generator (train)      | 6.20 | 1.23            | 4.97 \n",
            "generator (test)       | 5.46 | 0.22            | 5.24 \n",
            "discriminator (train)  | 3.88 | 0.46            | 3.41 \n",
            "discriminator (test)   | 5.91 | 0.87            | 5.04 \n",
            "(1010, 3, 100, 150)\n",
            "Epoch 5 of 100\n",
            "63/69 [==========================>...] - ETA: 28s"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "68/69 [============================>.] - ETA: 4s\n",
            "Testing for epoch 5:\n",
            "component              | loss | generation_loss | auxiliary_loss\n",
            "-----------------------------------------------------------------\n",
            "generator (train)      | 5.96 | 1.26            | 4.69 \n",
            "generator (test)       | 5.24 | 0.16            | 5.08 \n",
            "discriminator (train)  | 3.59 | 0.53            | 3.06 \n",
            "discriminator (test)   | 6.05 | 1.08            | 4.97 \n",
            "(1010, 3, 100, 150)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-177115993d64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    374\u001b[0m             \u001b[0mdis_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'params_discriminator_epoch_{0:03d}.hdf5'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;31m#             files.download(image_path)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m             \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m             \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdis_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'files' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "-bbShkT58s6U",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 717
        },
        "outputId": "130dc676-0fb3-424f-b902-538d9aab61dd",
        "executionInfo": {
          "status": "error",
          "timestamp": 1528571658356,
          "user_tz": -180,
          "elapsed": 1906,
          "user": {
            "displayName": "Kateryna Zorina",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "118398898453277774779"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "file: ACGAN - CIFAR10.py\n",
        "author: Luke\n",
        "de\n",
        "Oliveira(lukedeo @ vaitech.io)\n",
        "contributor: KnightTuYa(398225157 @ qq.com)\n",
        "Consult\n",
        "https: // github.com / lukedeo / keras - acgan\n",
        "for MNIST version!\n",
        "Consult\n",
        "https: // github.com / soumith / ganhacks\n",
        "for GAN trick!\n",
        "I directly use Minibatch Layer Code from:\n",
        "https://github.com/forcecore/Keras-GAN-Animeface-Character\n",
        "Thanks for the great work!\n",
        "I am still not satisfied with the generated images yet, Any suggestion is welcomed!\n",
        "\"\"\"\n",
        "from __future__ import print_function\n",
        "import os\n",
        "from collections import defaultdict\n",
        "\n",
        "try:\n",
        "    import cPickle as pickle\n",
        "except ImportError:\n",
        "    import pickle\n",
        "from PIL import Image\n",
        "from six.moves import range\n",
        "import keras.backend as K\n",
        "from keras.datasets import cifar10, mnist\n",
        "from keras import layers\n",
        "from keras.layers import Input, Dense, Reshape, Flatten, Embedding, Dropout, BatchNormalization\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers.convolutional import Conv2DTranspose, Conv2D\n",
        "from keras.models import Sequential, Model\n",
        "from keras.optimizers import Adam\n",
        "from keras.initializers import TruncatedNormal\n",
        "from keras.utils.generic_utils import Progbar\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.layers.noise import GaussianNoise\n",
        "import numpy as np\n",
        "\n",
        "np.random.seed(1337)\n",
        "class_num = 10\n",
        "K.set_image_dim_ordering('th')\n",
        "path = \"images\"  # The path to store the generated images\n",
        "load_weight = False\n",
        "# Set True if you need to reload weight\n",
        "load_epoch = 73  # Decide which epoch to reload weight, please check your file name\n",
        "\n",
        "def build_generator(latent_size):\n",
        "    # we will map a pair of (z, L), where z is a latent vector and L is a\n",
        "    # label drawn from P_c, to image space (..., 3, 32, 32)\n",
        "    cnn = Sequential()\n",
        "    cnn.add(Dense(384 * 7 * 7, input_dim=latent_size, activation='relu',\n",
        "                  kernel_initializer='glorot_normal', bias_initializer='Zeros'))\n",
        "    cnn.add(Reshape((384, 7, 7)))\n",
        "\n",
        "    cnn.add(Conv2DTranspose(96, kernel_size=5, strides=2, padding='same', activation='relu',\n",
        "                            kernel_initializer='glorot_normal', bias_initializer='Zeros'))\n",
        "    cnn.add(BatchNormalization())\n",
        "\n",
        "    cnn.add(Conv2DTranspose(1, kernel_size=5, strides=2, padding='same', activation='tanh',\n",
        "                            kernel_initializer='glorot_normal', bias_initializer='Zeros'))\n",
        "    cnn.summary()\n",
        "    # this is the z space commonly refered to in GAN papers\n",
        "    latent = Input(shape=(latent_size,))\n",
        "\n",
        "    # this will be our label\n",
        "    image_class = Input(shape=(1,), dtype='int32')\n",
        "\n",
        "    # 10 classes in CIFAR-10\n",
        "    cls = Flatten()(Embedding(101, latent_size,\n",
        "                              embeddings_initializer='glorot_normal')(image_class))\n",
        "\n",
        "    # hadamard product between z-space and a class conditional embedding\n",
        "    h = layers.multiply([latent, cls])\n",
        "\n",
        "    fake_image = cnn(h)\n",
        "\n",
        "    return Model([latent, image_class], fake_image)\n",
        "\n",
        "\n",
        "def build_discriminator():\n",
        "    # build a relatively standard conv net, with LeakyReLUs as suggested in\n",
        "    # the reference paper\n",
        "    cnn = Sequential()\n",
        "\n",
        "    cnn.add(GaussianNoise(0.05, input_shape=(28, 28)))  # Add this layer to prevent D from overfitting!\n",
        "\n",
        "    cnn.add(Conv2D(16, kernel_size=3, strides=2, padding='same',\n",
        "                   kernel_initializer='glorot_normal', bias_initializer='Zeros'))\n",
        "    cnn.add(LeakyReLU(alpha=0.2))\n",
        "    cnn.add(Dropout(0.5))\n",
        "\n",
        "    cnn.add(Conv2D(32, kernel_size=3, strides=1, padding='same',\n",
        "                   kernel_initializer='glorot_normal', bias_initializer='Zeros'))\n",
        "    cnn.add(BatchNormalization())\n",
        "    cnn.add(LeakyReLU(alpha=0.2))\n",
        "    cnn.add(Dropout(0.5))\n",
        "\n",
        "    cnn.add(Conv2D(64, kernel_size=3, strides=2, padding='same',\n",
        "                   kernel_initializer='glorot_normal', bias_initializer='Zeros'))\n",
        "    cnn.add(BatchNormalization())\n",
        "    cnn.add(LeakyReLU(alpha=0.2))\n",
        "    cnn.add(Dropout(0.5))\n",
        "\n",
        "    cnn.add(Conv2D(128, kernel_size=3, strides=1, padding='same',\n",
        "                   kernel_initializer='glorot_normal', bias_initializer='Zeros'))\n",
        "    cnn.add(BatchNormalization())\n",
        "    cnn.add(LeakyReLU(alpha=0.2))\n",
        "    cnn.add(Dropout(0.5))\n",
        "\n",
        "    cnn.add(Conv2D(256, kernel_size=3, strides=2, padding='same',\n",
        "                   kernel_initializer='glorot_normal', bias_initializer='Zeros'))\n",
        "    cnn.add(BatchNormalization())\n",
        "    cnn.add(LeakyReLU(alpha=0.2))\n",
        "    cnn.add(Dropout(0.5))\n",
        "\n",
        "    cnn.add(Conv2D(512, kernel_size=3, strides=1, padding='same',\n",
        "                   kernel_initializer='glorot_normal', bias_initializer='Zeros'))\n",
        "    cnn.add(BatchNormalization())\n",
        "    cnn.add(LeakyReLU(alpha=0.2))\n",
        "    cnn.add(Dropout(0.5))\n",
        "\n",
        "    cnn.add(Flatten())\n",
        "\n",
        "    cnn.add(MinibatchDiscrimination(50, 30))\n",
        "\n",
        "    image = Input(shape=(28, 28))\n",
        "\n",
        "    features = cnn(image)\n",
        "\n",
        "    # first output (name=generation) is whether or not the discriminator\n",
        "    # thinks the image that is being shown is fake, and the second output\n",
        "    # (name=auxiliary) is the class that the discriminator thinks the image\n",
        "    # belongs to.\n",
        "    fake = Dense(1, activation='sigmoid', name='generation',\n",
        "                 kernel_initializer='glorot_normal', bias_initializer='Zeros')(features)\n",
        "    aux = Dense(class_num, activation='softmax', name='auxiliary',\n",
        "                kernel_initializer='glorot_normal', bias_initializer='Zeros')(features)\n",
        "\n",
        "    return Model(image, [fake, aux])\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    # batch and latent size taken from the paper\n",
        "    nb_epochs = 100\n",
        "    batch_size = 100\n",
        "    latent_size = 110\n",
        "\n",
        "    # Adam parameters suggested in https://arxiv.org/abs/1511.06434\n",
        "    adam_lr = 0.0002\n",
        "    adam_beta_1 = 0.5\n",
        "\n",
        "    # build the discriminator, Choose Adam as optimizer according to GANHACK\n",
        "    discriminator = build_discriminator()\n",
        "    discriminator.compile(\n",
        "        optimizer=Adam(lr=adam_lr, beta_1=adam_beta_1),\n",
        "        loss=['binary_crossentropy', 'sparse_categorical_crossentropy']\n",
        "    )\n",
        "    generator = build_generator(latent_size)\n",
        "\n",
        "    latent = Input(shape=(latent_size,))\n",
        "    image_class = Input(shape=(1,), dtype='int32')\n",
        "\n",
        "    # get a fake image\n",
        "    fake = generator([latent, image_class])\n",
        "    print(fake.shape)\n",
        "    # we only want to be able to train generator for the combined model\n",
        "    discriminator.trainable = False\n",
        "    fake, aux = discriminator(fake)\n",
        "    combined = Model([latent, image_class], [fake, aux])\n",
        "\n",
        "    combined.compile(\n",
        "        optimizer=Adam(lr=adam_lr, beta_1=adam_beta_1),\n",
        "        loss=['binary_crossentropy', 'sparse_categorical_crossentropy']\n",
        "    )\n",
        "\n",
        "    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "    x_train = x_train.reshape(x_train.shape[0], 1, 28, 28)\n",
        "    x_test = x_test.reshape(x_test.shape[0], 1, 28, 28)\n",
        "    X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
        "    X_test = (X_test.astype(np.float32) - 127.5) / 127.5\n",
        "    nb_train, nb_test = X_train.shape[0], X_test.shape[0]\n",
        "\n",
        "    train_history = defaultdict(list)\n",
        "    test_history = defaultdict(list)\n",
        "\n",
        "    if load_weight:\n",
        "        generator.load_weights('params_generator_epoch_{0:03d}.hdf5'.format(load_epoch))\n",
        "        discriminator.load_weights('params_discriminator_epoch_{0:03d}.hdf5'.format(load_epoch))\n",
        "    else:\n",
        "        load_epoch = 0\n",
        "\n",
        "    for epoch in range(nb_epochs):\n",
        "        print('Epoch {} of {}'.format(load_epoch + 1, nb_epochs))\n",
        "        load_epoch += 1\n",
        "        nb_batches = int(X_train.shape[0] / batch_size)\n",
        "        progress_bar = Progbar(target=nb_batches)\n",
        "\n",
        "        epoch_gen_loss = []\n",
        "        epoch_disc_loss = []\n",
        "\n",
        "        for index in range(nb_batches):\n",
        "            progress_bar.update(index)\n",
        "            # generate a new batch of noise\n",
        "            noise = np.random.normal(0, 0.5, (batch_size, latent_size))\n",
        "\n",
        "            # get a batch of real images\n",
        "            image_batch = X_train[index * batch_size:(index + 1) * batch_size]\n",
        "            label_batch = y_train[index * batch_size:(index + 1) * batch_size]\n",
        "\n",
        "            # sample some labels from p_c\n",
        "            sampled_labels = np.random.randint(0, class_num, batch_size)\n",
        "\n",
        "            # generate a batch of fake images, using the generated labels as a\n",
        "            # conditioner. We reshape the sampled labels to be\n",
        "            # (batch_size, 1) so that we can feed them into the embedding\n",
        "            # layer as a length one sequence\n",
        "            generated_images = generator.predict(\n",
        "                [noise, sampled_labels.reshape((-1, 1))], verbose=0)\n",
        "\n",
        "            disc_real_weight = [np.ones(batch_size), 2 * np.ones(batch_size)]\n",
        "            disc_fake_weight = [np.ones(batch_size), np.zeros(batch_size)]\n",
        "\n",
        "            # According to GANHACK, We training our ACGAN-CIFAR10 in Real->D, Fake->D,\n",
        "            # Noise->G, rather than traditional method: [Real, Fake]->D, Noise->G, actully,\n",
        "            # it really make sense!\n",
        "\n",
        "            for train_ix in range(3):\n",
        "                if index % 30 != 0:\n",
        "                    X_real = image_batch\n",
        "                    # Label Soomthing\n",
        "                    y_real = np.random.uniform(0.7, 1.2, size=(batch_size,))\n",
        "                    aux_y1 = label_batch.reshape(-1, )\n",
        "                    epoch_disc_loss.append(discriminator.train_on_batch(X_real, [y_real, aux_y1]))\n",
        "                    # Label Soomthing\n",
        "                    X_fake = generated_images\n",
        "                    y_fake = np.random.uniform(0.0, 0.3, size=(batch_size,))\n",
        "                    aux_y2 = sampled_labels\n",
        "\n",
        "                    # see if the discriminator can figure itself out...\n",
        "                    epoch_disc_loss.append(discriminator.train_on_batch(X_fake, [y_fake, aux_y2]))\n",
        "                else:\n",
        "                    # make the labels the noisy for the discriminator: occasionally flip the labels\n",
        "                    # when training the discriminator\n",
        "                    X_real = image_batch\n",
        "                    y_real = np.random.uniform(0.0, 0.3, size=(batch_size,))\n",
        "                    aux_y1 = label_batch.reshape(-1, )\n",
        "\n",
        "                    epoch_disc_loss.append(discriminator.train_on_batch(X_real, [y_real, aux_y1]))\n",
        "                    # Label Soomthing\n",
        "                    X_fake = generated_images\n",
        "                    y_fake = np.random.uniform(0.7, 1.2, size=(batch_size,))\n",
        "                    aux_y2 = sampled_labels\n",
        "\n",
        "                    # see if the discriminator can figure itself out...\n",
        "                    epoch_disc_loss.append(discriminator.train_on_batch(X_fake, [y_fake, aux_y2]))\n",
        "            # make new noise. we generate Guassian Noise rather than Uniform Noise according to GANHACK\n",
        "            noise = np.random.normal(0, 0.5, (2 * batch_size, latent_size))\n",
        "            sampled_labels = np.random.randint(0, class_num, 2 * batch_size)\n",
        "\n",
        "            # we want to train the generator to trick the discriminator\n",
        "            # For the generator, we want all the {fake, not-fake} labels to say\n",
        "            # not-fake\n",
        "            trick = np.random.uniform(0.7, 1.2, size=(2 * batch_size,))\n",
        "\n",
        "            epoch_gen_loss.append(combined.train_on_batch(\n",
        "                [noise, sampled_labels.reshape((-1, 1))], [trick, sampled_labels]))\n",
        "\n",
        "        print('\\nTesting for epoch {}:'.format(load_epoch))\n",
        "\n",
        "        # evaluate the testing loss here\n",
        "\n",
        "        # generate a new batch of noise\n",
        "        noise = np.random.normal(0, 0.5, (nb_test, latent_size))\n",
        "\n",
        "        # sample some labels from p_c and generate images from them\n",
        "        sampled_labels = np.random.randint(0, class_num, nb_test)\n",
        "        generated_images = generator.predict(\n",
        "            [noise, sampled_labels.reshape((-1, 1))], verbose=False)\n",
        "\n",
        "        X = np.concatenate((X_test, generated_images))\n",
        "        y = np.array([1] * nb_test + [0] * nb_test)\n",
        "        aux_y = np.concatenate((y_test.reshape(-1, ), sampled_labels), axis=0)\n",
        "\n",
        "        # see if the discriminator can figure itself out...\n",
        "        discriminator_test_loss = discriminator.evaluate(\n",
        "            X, [y, aux_y], verbose=False)\n",
        "\n",
        "        discriminator_train_loss = np.mean(np.array(epoch_disc_loss), axis=0)\n",
        "\n",
        "        # make new noise\n",
        "        noise = np.random.normal(0, 0.5, (2 * nb_test, latent_size))\n",
        "        sampled_labels = np.random.randint(0, class_num, 2 * nb_test)\n",
        "        trick = np.ones(2 * nb_test)\n",
        "        generator_test_loss = combined.evaluate(\n",
        "            [noise, sampled_labels.reshape((-1, 1))],\n",
        "            [trick, sampled_labels], verbose=False)\n",
        "\n",
        "        generator_train_loss = np.mean(np.array(epoch_gen_loss), axis=0)\n",
        "\n",
        "        # generate an epoch report on performance\n",
        "        train_history['generator'].append(generator_train_loss)\n",
        "        train_history['discriminator'].append(discriminator_train_loss)\n",
        "\n",
        "        test_history['generator'].append(generator_test_loss)\n",
        "        test_history['discriminator'].append(discriminator_test_loss)\n",
        "\n",
        "        print('{0:<22s} | {1:4s} | {2:15s} | {3:5s}'.format(\n",
        "            'component', *discriminator.metrics_names))\n",
        "        print('-' * 65)\n",
        "\n",
        "        ROW_FMT = '{0:<22s} | {1:<4.2f} | {2:<15.2f} | {3:<5.2f}'\n",
        "        print(ROW_FMT.format('generator (train)',\n",
        "                             *train_history['generator'][-1]))\n",
        "        print(ROW_FMT.format('generator (test)',\n",
        "                             *test_history['generator'][-1]))\n",
        "        print(ROW_FMT.format('discriminator (train)',\n",
        "                             *train_history['discriminator'][-1]))\n",
        "        print(ROW_FMT.format('discriminator (test)',\n",
        "                             *test_history['discriminator'][-1]))\n",
        "\n",
        "        # save weights every epoch\n",
        "        generator.save_weights(\n",
        "            'params_generator_epoch_{0:03d}.hdf5'.format(load_epoch), True)\n",
        "        discriminator.save_weights(\n",
        "            'params_discriminator_epoch_{0:03d}.hdf5'.format(load_epoch), True)\n",
        "\n",
        "        # generate some pictures to display\n",
        "        noise = np.random.normal(0, 0.5, (100, latent_size))\n",
        "        sampled_labels = np.array([\n",
        "            [i] * 10 for i in range(10)\n",
        "        ]).reshape(-1, 1)\n",
        "        generated_images = generator.predict([noise, sampled_labels]).transpose(0, 2, 3, 1)\n",
        "        generated_images = np.asarray((generated_images * 127.5 + 127.5).astype(np.uint8))\n",
        "\n",
        "\n",
        "        def vis_square(data, padsize=1, padval=0):\n",
        "\n",
        "            # force the number of filters to be square\n",
        "            n = int(np.ceil(np.sqrt(data.shape[0])))\n",
        "            padding = ((0, n ** 2 - data.shape[0]), (0, padsize), (0, padsize)) + ((0, 0),) * (data.ndim - 3)\n",
        "            data = np.pad(data, padding, mode='constant', constant_values=(padval, padval))\n",
        "\n",
        "            # tile the filters into an image\n",
        "            data = data.reshape((n, n) + data.shape[1:]).transpose((0, 2, 1, 3) + tuple(range(4, data.ndim + 1)))\n",
        "            data = data.reshape((n * data.shape[1], n * data.shape[3]) + data.shape[4:])\n",
        "            return data\n",
        "\n",
        "\n",
        "        img = vis_square(generated_images)\n",
        "        if not os.path.exists(path):\n",
        "            os.makedirs(path)\n",
        "        Image.fromarray(img).save(\n",
        "            'images/plot_epoch_{0:03d}_generated.png'.format(load_epoch))\n",
        "        \n",
        "        \n",
        "        if load_epoch % 5 == 0:\n",
        "            image_path = 'images/plot_epoch_{0:03d}_generated.png'.format(load_epoch)\n",
        "            gen_path = 'params_generator_epoch_{0:03d}.hdf5'.format(load_epoch)\n",
        "            dis_path = 'params_discriminator_epoch_{0:03d}.hdf5'.format(load_epoch)\n",
        "            files.download(image_path)\n",
        "            files.download(gen_path)\n",
        "            files.download(dis_path)\n",
        "        \n",
        "        pickle.dump({'train': train_history, 'test': test_history},\n",
        "                        open('acgan-history.pkl', 'wb'))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-b2b20441665f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;31m# build the discriminator, Choose Adam as optimizer according to GANHACK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m     \u001b[0mdiscriminator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_discriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m     discriminator.compile(\n\u001b[1;32m    160\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madam_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta_1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madam_beta_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-27-b2b20441665f>\u001b[0m in \u001b[0;36mbuild_discriminator\u001b[0;34m()\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     cnn.add(Conv2D(16, kernel_size=3, strides=2, padding='same',\n\u001b[0;32m---> 92\u001b[0;31m                    kernel_initializer='glorot_normal', bias_initializer='Zeros'))\n\u001b[0m\u001b[1;32m     93\u001b[0m     \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLeakyReLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/models.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    520\u001b[0m                           output_shapes=[self.outputs[0]._keras_shape])\n\u001b[1;32m    521\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 522\u001b[0;31m             \u001b[0moutput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m                 raise TypeError('All layers in a Sequential model '\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    573\u001b[0m                 \u001b[0;31m# Raise exceptions in case the input is not compatible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m                 \u001b[0;31m# with the input_spec specified in the layer constructor.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_input_compatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m                 \u001b[0;31m# Collect input shapes to build layer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    472\u001b[0m                                      \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m': expected ndim='\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m                                      \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m', found ndim='\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 474\u001b[0;31m                                      str(K.ndim(x)))\n\u001b[0m\u001b[1;32m    475\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_ndim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m                 \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input 0 is incompatible with layer conv2d_75: expected ndim=4, found ndim=3"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "mEO6eyW6_B1Z",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "97ce8e08-3e2d-4ab9-a879-c48691a732ff",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1528355919728,
          "user_tz": -180,
          "elapsed": 678,
          "user": {
            "displayName": "Kateryna Zorina",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "118398898453277774779"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "print(generated_images.shape)\n",
        "#print(image_path)\n",
        "#files.download(image_path)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(100, 3, 32, 32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fxRdtKWEhgf5",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ThXS9y6olNGl",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "generator.save('my_model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CrC_6YrbeVTP",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "image_path = 'images/plot_epoch_{0:03d}_generated.png'.format(load_epoch)\n",
        "            gen_path = 'params_generator_epoch_{0:03d}.hdf5'.format(load_epoch)\n",
        "            dis_path = 'params_discriminator_epoch_{0:03d}.hdf5'.format(load_epoch)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XrSCulDteVVr",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "files.download('cifar_model_050.hdf5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zIhjUSaLeVX7",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "files.download('images/plot_epoch_{0:03d}_generated.png'.format(load_epoch))\n",
        "files.download('params_generator_epoch_{0:03d}.hdf5'.format(load_epoch))\n",
        "files.download('params_discriminator_epoch_{0:03d}.hdf5'.format(load_epoch))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vYaCZGR-cc9O",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "eecf1ce7-841e-4462-f448-3d514e57922a",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1528230582099,
          "user_tz": -180,
          "elapsed": 1958,
          "user": {
            "displayName": "Kateryna Zorina",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "118398898453277774779"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "datalab\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "82qjDC3Qcdh8",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "1c7a0882-5556-4984-824e-058351e10df3",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1527657813415,
          "user_tz": -180,
          "elapsed": 2998,
          "user": {
            "displayName": "Kateryna Zorina",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "118398898453277774779"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!cd images\n",
        "!ls"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "acgan-history.pkl  params_discriminator_epoch_001.hdf5\r\n",
            "datalab\t\t   params_generator_epoch_001.hdf5\r\n",
            "images\t\t   your desired name.txt\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "R7y7TEWucgdu",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "noise = np.random.normal(0,1,32*32*3)\n",
        "img = noise.reshape(32,32,3)\n",
        "fig = plt.imshow(img)\n",
        "fig.savefig(\"test.png\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NEihOvEldxtk",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mFe88ipid4J2",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "text_file = open(\"images/plot_epoch_005_generated.png\", \"w\")\n",
        "text_file.write(\"hello world\")    \n",
        "text_file.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Dl_m8tiSeFy0",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yCKBwMogeJOZ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "98caf82d-bc28-4edd-9aa6-29e059de4b29",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1528059952872,
          "user_tz": -180,
          "elapsed": 2059,
          "user": {
            "displayName": "Kateryna Zorina",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "118398898453277774779"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!ls\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "datalab\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "you9_5MzHafr",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras.datasets import cifar10\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "\n",
        "\n",
        "NUM_CLASSES = 10\n",
        "BATCH_SIZE = 32\n",
        "NUM_EPOCHS = 50\n",
        "original_dim = 32*32*2\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MDhUyKH-QT_N",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "df942f3a-8ee6-4387-c332-d0a502ad003e",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1528368297784,
          "user_tz": -180,
          "elapsed": 785,
          "user": {
            "displayName": "Kateryna Zorina",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "118398898453277774779"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (50000, 3, 32, 32)\n",
            "50000 train samples\n",
            "10000 test samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "f8cHX2mceMt2",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "0e7aea80-1c73-40ba-b2d2-a7bb96e11487",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1528368301705,
          "user_tz": -180,
          "elapsed": 573,
          "user": {
            "displayName": "Kateryna Zorina",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "118398898453277774779"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "y_train = keras.utils.to_categorical(y_train, NUM_CLASSES)\n",
        "y_test = keras.utils.to_categorical(y_test, NUM_CLASSES)\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, kernel_size=(3, 3),\n",
        "                 padding='same',\n",
        "                 activation='relu',\n",
        "                 input_shape=x_train.shape[1:]))\n",
        "model.add(Conv2D(32, kernel_size=(3, 3),\n",
        "                 activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Conv2D(64, (3, 3),\n",
        "                 padding='same',\n",
        "                 activation='relu'))\n",
        "model.add(Conv2D(64, (3, 3),\n",
        "                 activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
        "\n",
        "model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "              optimizer=keras.optimizers.Adam(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(50000, 3, 32, 32)\n",
            "(10000, 3, 32, 32)\n",
            "(50000, 10)\n",
            "(10000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UraH6C0kInnj",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uYK3P1dywxbq",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 1747
        },
        "outputId": "00d1aa55-de1f-4336-f111-396910d6ad39",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1528363022195,
          "user_tz": -180,
          "elapsed": 1219247,
          "user": {
            "displayName": "Kateryna Zorina",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "118398898453277774779"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "model.fit(x_train, y_train,\n",
        "          batch_size=BATCH_SIZE,\n",
        "          epochs=NUM_EPOCHS,\n",
        "          verbose=2,\n",
        "          validation_data=(x_test, y_test))\n",
        "model.save_weights('pure_cifar_weights_50_epoch.hdf5')\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/50\n",
            " - 25s - loss: 14.4942 - acc: 0.1002 - val_loss: 14.5063 - val_acc: 0.1000\n",
            "Epoch 2/50\n",
            " - 24s - loss: 2.2882 - acc: 0.6446 - val_loss: 0.7767 - val_acc: 0.7683\n",
            "Epoch 3/50\n",
            " - 24s - loss: 0.4484 - acc: 0.8453 - val_loss: 0.7175 - val_acc: 0.7780\n",
            "Epoch 4/50\n",
            " - 24s - loss: 0.4150 - acc: 0.8555 - val_loss: 0.7620 - val_acc: 0.7824\n",
            "Epoch 5/50\n",
            " - 24s - loss: 0.4068 - acc: 0.8599 - val_loss: 0.7774 - val_acc: 0.7703\n",
            "Epoch 6/50\n",
            " - 24s - loss: 0.4000 - acc: 0.8625 - val_loss: 0.7300 - val_acc: 0.7830\n",
            "Epoch 7/50\n",
            " - 24s - loss: 0.3957 - acc: 0.8616 - val_loss: 0.7272 - val_acc: 0.7824\n",
            "Epoch 8/50\n",
            " - 24s - loss: 0.3891 - acc: 0.8658 - val_loss: 0.7448 - val_acc: 0.7882\n",
            "Epoch 9/50\n",
            " - 24s - loss: 0.3902 - acc: 0.8659 - val_loss: 0.7396 - val_acc: 0.7807\n",
            "Epoch 10/50\n",
            " - 24s - loss: 0.3846 - acc: 0.8665 - val_loss: 0.7499 - val_acc: 0.7801\n",
            "Epoch 11/50\n",
            " - 24s - loss: 0.3839 - acc: 0.8667 - val_loss: 0.7176 - val_acc: 0.7855\n",
            "Epoch 12/50\n",
            " - 24s - loss: 0.3835 - acc: 0.8662 - val_loss: 0.7372 - val_acc: 0.7831\n",
            "Epoch 13/50\n",
            " - 24s - loss: 0.3738 - acc: 0.8691 - val_loss: 0.7344 - val_acc: 0.7813\n",
            "Epoch 14/50\n",
            " - 24s - loss: 0.3662 - acc: 0.8740 - val_loss: 0.7317 - val_acc: 0.7862\n",
            "Epoch 15/50\n",
            " - 24s - loss: 0.3769 - acc: 0.8702 - val_loss: 0.7255 - val_acc: 0.7833\n",
            "Epoch 16/50\n",
            " - 24s - loss: 0.3685 - acc: 0.8725 - val_loss: 0.7382 - val_acc: 0.7876\n",
            "Epoch 17/50\n",
            " - 24s - loss: 0.3681 - acc: 0.8752 - val_loss: 0.7480 - val_acc: 0.7809\n",
            "Epoch 18/50\n",
            " - 24s - loss: 0.3614 - acc: 0.8755 - val_loss: 0.7522 - val_acc: 0.7774\n",
            "Epoch 19/50\n",
            " - 24s - loss: 0.3678 - acc: 0.8718 - val_loss: 0.7302 - val_acc: 0.7880\n",
            "Epoch 20/50\n",
            " - 24s - loss: 0.3676 - acc: 0.8731 - val_loss: 0.7038 - val_acc: 0.7921\n",
            "Epoch 21/50\n",
            " - 24s - loss: 0.3667 - acc: 0.8742 - val_loss: 0.7494 - val_acc: 0.7847\n",
            "Epoch 22/50\n",
            " - 24s - loss: 0.3584 - acc: 0.8765 - val_loss: 0.7382 - val_acc: 0.7809\n",
            "Epoch 23/50\n",
            " - 25s - loss: 0.3550 - acc: 0.8766 - val_loss: 0.7738 - val_acc: 0.7810\n",
            "Epoch 24/50\n",
            " - 24s - loss: 0.3648 - acc: 0.8729 - val_loss: 0.7740 - val_acc: 0.7853\n",
            "Epoch 25/50\n",
            " - 25s - loss: 0.3617 - acc: 0.8756 - val_loss: 0.7685 - val_acc: 0.7840\n",
            "Epoch 26/50\n",
            " - 25s - loss: 0.3535 - acc: 0.8783 - val_loss: 0.7429 - val_acc: 0.7839\n",
            "Epoch 27/50\n",
            " - 24s - loss: 0.3580 - acc: 0.8760 - val_loss: 0.7549 - val_acc: 0.7762\n",
            "Epoch 28/50\n",
            " - 24s - loss: 0.3574 - acc: 0.8790 - val_loss: 0.7699 - val_acc: 0.7815\n",
            "Epoch 29/50\n",
            " - 24s - loss: 0.3585 - acc: 0.8768 - val_loss: 0.7590 - val_acc: 0.7791\n",
            "Epoch 30/50\n",
            " - 25s - loss: 0.3492 - acc: 0.8798 - val_loss: 0.7321 - val_acc: 0.7877\n",
            "Epoch 31/50\n",
            " - 24s - loss: 0.3520 - acc: 0.8801 - val_loss: 0.7569 - val_acc: 0.7870\n",
            "Epoch 32/50\n",
            " - 24s - loss: 0.3510 - acc: 0.8806 - val_loss: 0.7761 - val_acc: 0.7848\n",
            "Epoch 33/50\n",
            " - 24s - loss: 0.3494 - acc: 0.8827 - val_loss: 0.7937 - val_acc: 0.7796\n",
            "Epoch 34/50\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " - 24s - loss: 0.3444 - acc: 0.8819 - val_loss: 0.7612 - val_acc: 0.7830\n",
            "Epoch 35/50\n",
            " - 24s - loss: 0.3470 - acc: 0.8820 - val_loss: 0.8287 - val_acc: 0.7805\n",
            "Epoch 36/50\n",
            " - 24s - loss: 0.3478 - acc: 0.8807 - val_loss: 0.7566 - val_acc: 0.7854\n",
            "Epoch 37/50\n",
            " - 24s - loss: 0.3390 - acc: 0.8836 - val_loss: 0.7652 - val_acc: 0.7845\n",
            "Epoch 38/50\n",
            " - 24s - loss: 0.3414 - acc: 0.8836 - val_loss: 0.7443 - val_acc: 0.7817\n",
            "Epoch 39/50\n",
            " - 24s - loss: 0.3417 - acc: 0.8830 - val_loss: 0.7697 - val_acc: 0.7874\n",
            "Epoch 40/50\n",
            " - 24s - loss: 0.3353 - acc: 0.8848 - val_loss: 0.8153 - val_acc: 0.7811\n",
            "Epoch 41/50\n",
            " - 24s - loss: 0.3367 - acc: 0.8848 - val_loss: 0.7618 - val_acc: 0.7870\n",
            "Epoch 42/50\n",
            " - 24s - loss: 0.3440 - acc: 0.8824 - val_loss: 0.8251 - val_acc: 0.7703\n",
            "Epoch 43/50\n",
            " - 24s - loss: 0.3358 - acc: 0.8856 - val_loss: 0.7760 - val_acc: 0.7826\n",
            "Epoch 44/50\n",
            " - 24s - loss: 0.3316 - acc: 0.8872 - val_loss: 0.7759 - val_acc: 0.7802\n",
            "Epoch 45/50\n",
            " - 24s - loss: 0.3352 - acc: 0.8866 - val_loss: 0.7561 - val_acc: 0.7883\n",
            "Epoch 46/50\n",
            " - 24s - loss: 0.3408 - acc: 0.8841 - val_loss: 0.7744 - val_acc: 0.7764\n",
            "Epoch 47/50\n",
            " - 24s - loss: 0.3276 - acc: 0.8885 - val_loss: 0.7725 - val_acc: 0.7887\n",
            "Epoch 48/50\n",
            " - 24s - loss: 0.3327 - acc: 0.8875 - val_loss: 0.7418 - val_acc: 0.7885\n",
            "Epoch 49/50\n",
            " - 25s - loss: 0.3323 - acc: 0.8889 - val_loss: 0.8184 - val_acc: 0.7791\n",
            "Epoch 50/50\n",
            " - 24s - loss: 0.3302 - acc: 0.8878 - val_loss: 0.7487 - val_acc: 0.7795\n",
            "Test loss: 0.7487001701831818\n",
            "Test accuracy: 0.7795\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mNvNk9io8eMs",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "model.load_weights('pure_cifar_weights_50_epoch.hdf5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6nRS92cG2UOE",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "076d4793-b3b0-40b2-a462-26f67382e379",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1528368319263,
          "user_tz": -180,
          "elapsed": 2041,
          "user": {
            "displayName": "Kateryna Zorina",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "118398898453277774779"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test loss: 0.7487001701831818\n",
            "Test accuracy: 0.7795\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fcQHiAtJdf0n",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "fb16d3a6-44f4-48ea-cdb9-203c995f3245",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1528367922624,
          "user_tz": -180,
          "elapsed": 695,
          "user": {
            "displayName": "Kateryna Zorina",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "118398898453277774779"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "print(np.unique(y_train))"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 1 2 3 4 5 6 7 8 9]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hsq1QIj_6CxL",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "9cce4fc4-0df1-4cca-d52e-5d17e44cfc82",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1528359376170,
          "user_tz": -180,
          "elapsed": 4410,
          "user": {
            "displayName": "Kateryna Zorina",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "118398898453277774779"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "noise = np.random.normal(0, 0.5, (10000, latent_size))\n",
        "sampled_labels = np.array([\n",
        "    [i] * 1000 for i in range(10)\n",
        "]).reshape(-1, 1)\n",
        "generated_images = generator.predict([noise, sampled_labels])#.transpose(0, 2, 3, 1)\n",
        "generated_images = np.asarray((generated_images * 127.5 + 127.5).astype(np.uint8))\n",
        "\n",
        "#print(generated_images.shape)\n",
        "print(x_train.shape)\n",
        "print(generated_images.shape)\n",
        "#generated_images =generated_images.transpose([1, 2, 0])\n",
        "#print(generated_images.shape)\n",
        "sampled_labels = np.array([\n",
        "            [i] * 1000 for i in range(10)\n",
        "        ])\n",
        "sampled_labels = sampled_labels.reshape(10000)\n",
        "print(sampled_labels)\n",
        "sampled_labels = keras.utils.to_categorical(sampled_labels, NUM_CLASSES)\n",
        "print(sampled_labels.shape)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(50000, 3, 32, 32)\n",
            "(10000, 3, 32, 32)\n",
            "(10000, 3, 32, 32)\n",
            "[0 0 0 ... 9 9 9]\n",
            "(10000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VY8wNO8JHi0E",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "outputId": "75e90a41-99e5-41a3-b16c-4d74d0127459",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1528359407945,
          "user_tz": -180,
          "elapsed": 506,
          "user": {
            "displayName": "Kateryna Zorina",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "118398898453277774779"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "plt.imshow(generated_images[4011].reshape(3,32,32).transpose([1, 2, 0]))\n",
        "plt.show\n",
        "\n"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function matplotlib.pyplot.show>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUsAAAFKCAYAAACU6307AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nXcRDADAAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f807eae1550>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "Jdjz0YAlRFBS",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "outputId": "11e463c2-2b73-4b55-9bfd-e10e22100a13",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1528359305856,
          "user_tz": -180,
          "elapsed": 499,
          "user": {
            "displayName": "Kateryna Zorina",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "118398898453277774779"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "plt.imshow(x_train[0].reshape(3,32,32).transpose([1, 2, 0]))\n",
        "plt.show"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function matplotlib.pyplot.show>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUsAAAFKCAYAAACU6307AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIlJgsCQiUmCwJCJSYLAkIlL4/wKvOysvkgncAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f807ebedb00>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "dnbCT3Tb_oCE",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "filepath=\"weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PkSjl24pwyxk",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 1723
        },
        "outputId": "3ca04a53-bf7b-4128-f374-27d795be44f4",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1528371967615,
          "user_tz": -180,
          "elapsed": 1480043,
          "user": {
            "displayName": "Kateryna Zorina",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "118398898453277774779"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "model.fit(augmented, augmented_y,\n",
        "         batch_size=BATCH_SIZE,\n",
        "          epochs=50,\n",
        "          verbose=2,\n",
        "          validation_data=(x_test, y_test),\n",
        "         callbacks=callbacks_list)\n",
        "         "
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/50\n",
            " - 30s - loss: 2.8843 - acc: 0.6344 - val_loss: 1.0093 - val_acc: 0.6685\n",
            "Epoch 2/50\n",
            " - 29s - loss: 2.8915 - acc: 0.6718 - val_loss: 0.7783 - val_acc: 0.7745\n",
            "Epoch 3/50\n",
            " - 29s - loss: 3.1602 - acc: 0.5759 - val_loss: 0.8235 - val_acc: 0.7454\n",
            "Epoch 4/50\n",
            " - 29s - loss: 2.9134 - acc: 0.6827 - val_loss: 0.7710 - val_acc: 0.7658\n",
            "Epoch 5/50\n",
            " - 29s - loss: 3.1155 - acc: 0.5877 - val_loss: 0.8456 - val_acc: 0.7250\n",
            "Epoch 6/50\n",
            " - 29s - loss: 3.1858 - acc: 0.5621 - val_loss: 0.8917 - val_acc: 0.7005\n",
            "Epoch 7/50\n",
            " - 30s - loss: 3.0051 - acc: 0.6293 - val_loss: 0.8925 - val_acc: 0.6972\n",
            "Epoch 8/50\n",
            " - 30s - loss: 3.0212 - acc: 0.6168 - val_loss: 0.9329 - val_acc: 0.7046\n",
            "Epoch 9/50\n",
            " - 30s - loss: 3.0406 - acc: 0.6302 - val_loss: 0.8107 - val_acc: 0.7458\n",
            "Epoch 10/50\n",
            " - 30s - loss: 2.9209 - acc: 0.6626 - val_loss: 0.8187 - val_acc: 0.7452\n",
            "Epoch 11/50\n",
            " - 30s - loss: 2.9265 - acc: 0.6519 - val_loss: 0.8229 - val_acc: 0.7495\n",
            "Epoch 12/50\n",
            " - 29s - loss: 2.9596 - acc: 0.6457 - val_loss: 1.0001 - val_acc: 0.6961\n",
            "Epoch 13/50\n",
            " - 29s - loss: 2.9714 - acc: 0.6547 - val_loss: 0.7939 - val_acc: 0.7553\n",
            "Epoch 14/50\n",
            " - 30s - loss: 2.9705 - acc: 0.6465 - val_loss: 0.8301 - val_acc: 0.7442\n",
            "Epoch 15/50\n",
            " - 30s - loss: 3.1518 - acc: 0.5816 - val_loss: 0.8314 - val_acc: 0.7433\n",
            "Epoch 16/50\n",
            " - 29s - loss: 2.8288 - acc: 0.6816 - val_loss: 0.7614 - val_acc: 0.7595\n",
            "Epoch 17/50\n",
            " - 29s - loss: 2.8597 - acc: 0.6896 - val_loss: 0.7675 - val_acc: 0.7607\n",
            "Epoch 18/50\n",
            " - 30s - loss: 2.8959 - acc: 0.6713 - val_loss: 0.7956 - val_acc: 0.7574\n",
            "Epoch 19/50\n",
            " - 30s - loss: 3.0137 - acc: 0.6206 - val_loss: 1.2183 - val_acc: 0.5954\n",
            "Epoch 20/50\n",
            " - 30s - loss: 1.8011 - acc: 0.4624 - val_loss: 0.8389 - val_acc: 0.7321\n",
            "Epoch 21/50\n",
            " - 29s - loss: 1.0748 - acc: 0.6283 - val_loss: 0.8172 - val_acc: 0.7357\n",
            "Epoch 22/50\n",
            " - 30s - loss: 0.9341 - acc: 0.6723 - val_loss: 0.7842 - val_acc: 0.7532\n",
            "Epoch 23/50\n",
            " - 29s - loss: 0.8873 - acc: 0.6813 - val_loss: 0.7880 - val_acc: 0.7537\n",
            "Epoch 24/50\n",
            " - 30s - loss: 0.8816 - acc: 0.6987 - val_loss: 0.7608 - val_acc: 0.7590\n",
            "Epoch 25/50\n",
            " - 29s - loss: 0.8392 - acc: 0.7077 - val_loss: 0.7742 - val_acc: 0.7650\n",
            "Epoch 26/50\n",
            " - 30s - loss: 0.8003 - acc: 0.7107 - val_loss: 0.8287 - val_acc: 0.7573\n",
            "Epoch 27/50\n",
            " - 30s - loss: 0.8579 - acc: 0.7071 - val_loss: 0.7536 - val_acc: 0.7726\n",
            "Epoch 28/50\n",
            " - 29s - loss: 0.8369 - acc: 0.7134 - val_loss: 0.7611 - val_acc: 0.7753\n",
            "Epoch 29/50\n",
            " - 30s - loss: 0.7836 - acc: 0.7184 - val_loss: 0.7385 - val_acc: 0.7727\n",
            "Epoch 30/50\n",
            " - 30s - loss: 0.7855 - acc: 0.7227 - val_loss: 0.7740 - val_acc: 0.7493\n",
            "Epoch 31/50\n",
            " - 30s - loss: 0.7615 - acc: 0.7223 - val_loss: 0.7606 - val_acc: 0.7728\n",
            "Epoch 32/50\n",
            " - 30s - loss: 0.7527 - acc: 0.7256 - val_loss: 0.7540 - val_acc: 0.7728\n",
            "Epoch 33/50\n",
            " - 30s - loss: 0.7501 - acc: 0.7267 - val_loss: 0.7672 - val_acc: 0.7756\n",
            "Epoch 34/50\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " - 30s - loss: 0.7606 - acc: 0.7242 - val_loss: 0.8304 - val_acc: 0.7365\n",
            "Epoch 35/50\n",
            " - 30s - loss: 0.7662 - acc: 0.7206 - val_loss: 0.8139 - val_acc: 0.7714\n",
            "Epoch 36/50\n",
            " - 30s - loss: 0.7395 - acc: 0.7322 - val_loss: 0.8228 - val_acc: 0.7594\n",
            "Epoch 37/50\n",
            " - 30s - loss: 0.7363 - acc: 0.7315 - val_loss: 0.7846 - val_acc: 0.7751\n",
            "Epoch 38/50\n",
            " - 30s - loss: 0.7262 - acc: 0.7333 - val_loss: 0.7584 - val_acc: 0.7748\n",
            "Epoch 39/50\n",
            " - 30s - loss: 0.7347 - acc: 0.7313 - val_loss: 0.8239 - val_acc: 0.7722\n",
            "Epoch 40/50\n",
            " - 30s - loss: 0.7363 - acc: 0.7328 - val_loss: 0.7798 - val_acc: 0.7721\n",
            "Epoch 41/50\n",
            " - 30s - loss: 0.7264 - acc: 0.7353 - val_loss: 0.7843 - val_acc: 0.7773\n",
            "Epoch 42/50\n",
            " - 30s - loss: 0.7228 - acc: 0.7355 - val_loss: 0.7748 - val_acc: 0.7750\n",
            "Epoch 43/50\n",
            " - 30s - loss: 0.7165 - acc: 0.7368 - val_loss: 0.7816 - val_acc: 0.7725\n",
            "Epoch 44/50\n",
            " - 30s - loss: 0.7189 - acc: 0.7367 - val_loss: 0.7828 - val_acc: 0.7787\n",
            "Epoch 45/50\n",
            " - 30s - loss: 0.7134 - acc: 0.7405 - val_loss: 0.7655 - val_acc: 0.7783\n",
            "Epoch 46/50\n",
            " - 30s - loss: 0.7406 - acc: 0.7352 - val_loss: 0.7965 - val_acc: 0.7708\n",
            "Epoch 47/50\n",
            " - 30s - loss: 0.7269 - acc: 0.7388 - val_loss: 0.7873 - val_acc: 0.7750\n",
            "Epoch 48/50\n",
            " - 30s - loss: 0.7009 - acc: 0.7443 - val_loss: 0.8199 - val_acc: 0.7716\n",
            "Epoch 49/50\n",
            " - 30s - loss: 0.7134 - acc: 0.7399 - val_loss: 0.7860 - val_acc: 0.7754\n",
            "Epoch 50/50\n",
            " - 30s - loss: 0.7041 - acc: 0.7432 - val_loss: 0.7853 - val_acc: 0.7746\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f819ba25550>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "metadata": {
        "id": "Q-PUzU_m9tq7",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "21d58c2e-5b64-4420-9564-a202e8422ab7",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1528370423701,
          "user_tz": -180,
          "elapsed": 563,
          "user": {
            "displayName": "Kateryna Zorina",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "118398898453277774779"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "print(type(x_train))\n",
        "print(type(generated_images))\n",
        "augmented = np.concatenate((x_train, generated_images))\n",
        "print(augmented.shape)\n",
        "augmented_y = np.concatenate((y_train, sampled_labels))\n",
        "print(augmented_y.shape)"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "(60000, 3, 32, 32)\n",
            "(60000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FbNe24FI7nrW",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "5e89b733-72de-4843-ecca-7b0f501a81fa",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1528375432235,
          "user_tz": -180,
          "elapsed": 2054,
          "user": {
            "displayName": "Kateryna Zorina",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "118398898453277774779"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test loss: 0.7853004289865494\n",
            "Test accuracy: 0.7746\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tiwhUK2fwrn1",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# generate some pictures to display\n",
        "noise = np.random.normal(0, 0.5, (10000, latent_size))\n",
        "sampled_labels = np.array([\n",
        "    [i] * 1000 for i in range(10)\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JUwS4slrw-_D",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "sampled_labels = sampled_labels.reshape(-1, 1)\n",
        "generated_images = generator.predict([noise, sampled_labels]).transpose(0, 2, 3, 1)\n",
        "generated_images = np.asarray((generated_images * 127.5 + 127.5).astype(np.uint8))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mMPnguxJSAZS",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "e8474aba-a3d4-4c32-f280-0b64597406f8"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-bb64ff1c-99f4-4480-a522-610902f871af\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-bb64ff1c-99f4-4480-a522-610902f871af\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving caltech_dataset.pickle to caltech_dataset.pickle\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8v97F6SK5On4",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zVFRcXNQSBG0",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "with open('caltech_dataset.pickle', 'rb') as data:\n",
        "    X_train, X_test, y_train, y_test = pickle.load(data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nTNqfu8HSip7",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "x_train = X_train.astype('float32').transpose(0, 2, 3, 1)\n",
        "x_test = X_test.astype('float32').transpose(0, 2, 3, 1)\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uJryazhuTTro",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras.datasets import cifar10, mnist\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "import pickle\n",
        "\n",
        "def build_model(data_augmentation, dataset):\n",
        "    BATCH_SIZE = 32\n",
        "    NUM_EPOCHS = 2\n",
        "    NUM_CLASSES = 10\n",
        "    if (dataset == 'mnist'):\n",
        "        (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "        x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
        "        x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
        "        x_train = x_train.astype('float32')\n",
        "        x_test = x_test.astype('float32')\n",
        "    elif (dataset == 'cifar10'):\n",
        "        (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "        x_train = x_train.astype('float32')\n",
        "        x_test = x_test.astype('float32')\n",
        "    elif (dataset == 'caltech101'):\n",
        "        NUM_CLASSES = 101\n",
        "        with open('caltech_dataset.pickle', 'rb') as data:\n",
        "            X_train, X_test, y_train, y_test = pickle.load(data)\n",
        "        x_train = X_train.astype('float32')\n",
        "        x_test = X_test.astype('float32')\n",
        "\n",
        "\n",
        "    x_train /= 255\n",
        "    x_test /= 255\n",
        "    print('x_train shape:', x_train.shape)\n",
        "    print(x_train.shape[0], 'train samples')\n",
        "    print(x_test.shape[0], 'test samples')\n",
        "    y_train = keras.utils.to_categorical(y_train, NUM_CLASSES)\n",
        "    y_test = keras.utils.to_categorical(y_test, NUM_CLASSES)\n",
        "    print(x_train.shape)\n",
        "    print(x_test.shape)\n",
        "    print(y_train.shape)\n",
        "    print(y_test.shape)\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(32, kernel_size=(3, 3),\n",
        "                     padding='same',\n",
        "                     activation='relu',\n",
        "                     input_shape=x_train.shape[1:]))\n",
        "    model.add(Conv2D(32, kernel_size=(3, 3),\n",
        "                     activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Conv2D(64, (3, 3),\n",
        "                     padding='same',\n",
        "                     activation='relu'))\n",
        "    model.add(Conv2D(64, (3, 3),\n",
        "                     activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(512, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
        "\n",
        "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "                  optimizer=keras.optimizers.Adam(),\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    model.summary()\n",
        "    return model\n",
        "#     model.fit(x_train, y_train,\n",
        "#               batch_size=BATCH_SIZE,\n",
        "#               epochs=NUM_EPOCHS,\n",
        "#               verbose=2,\n",
        "#               validation_data=(x_test, y_test))\n",
        "#     score = model.evaluate(x_test, y_test, verbose=0)\n",
        "#     print('Test loss:', score[0])\n",
        "#     print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FJDPnqE-48Nb",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "try:\n",
        "    import cPickle as pickle\n",
        "except ImportError:\n",
        "    import pickle\n",
        "from PIL import Image\n",
        "from six.moves import range\n",
        "import keras.backend as K\n",
        "from keras.datasets import cifar10, mnist\n",
        "from keras import layers\n",
        "from keras.layers import Input, Dense, Reshape, Flatten, Embedding, Dropout, BatchNormalization\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers.convolutional import Conv2DTranspose, Conv2D\n",
        "from keras.models import Sequential, Model\n",
        "from keras.optimizers import Adam\n",
        "from keras.initializers import TruncatedNormal\n",
        "from keras.utils.generic_utils import Progbar\n",
        "\n",
        "def build_generator(latent_size):\n",
        "    # we will map a pair of (z, L), where z is a latent vector and L is a\n",
        "    # label drawn from P_c, to image space (..., 3, 32, 32)\n",
        "    cnn = Sequential()\n",
        "    cnn.add(Dense(384 * 4 * 4, input_dim=latent_size, activation='relu',\n",
        "                  kernel_initializer='glorot_normal', bias_initializer='Zeros'))\n",
        "    cnn.add(Reshape((384, 4, 4)))\n",
        "    cnn.add(Conv2DTranspose(192, kernel_size=5, strides=2, padding='same', activation='relu',\n",
        "                            kernel_initializer='glorot_normal', bias_initializer='Zeros'))\n",
        "    cnn.add(BatchNormalization())\n",
        "    cnn.add(Conv2DTranspose(96, kernel_size=5, strides=2, padding='same', activation='relu',\n",
        "                            kernel_initializer='glorot_normal', bias_initializer='Zeros'))\n",
        "    cnn.add(BatchNormalization())\n",
        "    cnn.add(Conv2DTranspose(3, kernel_size=5, strides=2, padding='same', activation='tanh',\n",
        "                            kernel_initializer='glorot_normal', bias_initializer='Zeros'))\n",
        "\n",
        "    # this is the z space commonly refered to in GAN papers\n",
        "    latent = Input(shape=(latent_size,))\n",
        "\n",
        "    # this will be our label\n",
        "    image_class = Input(shape=(1,), dtype='int32')\n",
        "\n",
        "    # 10 classes in CIFAR-10\n",
        "    cls = Flatten()(Embedding(10, latent_size,\n",
        "                              embeddings_initializer='glorot_normal')(image_class))\n",
        "\n",
        "    # hadamard product between z-space and a class conditional embedding\n",
        "    h = layers.multiply([latent, cls])\n",
        "\n",
        "    fake_image = cnn(h)\n",
        "\n",
        "    return Model([latent, image_class], fake_image)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4nzu1AO543Gc",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "latent_size = 110\n",
        "weights_path = 'params_generator_epoch_073.hdf5'\n",
        "def generate_images(amount):\n",
        "    generator = build_generator(latent_size)\n",
        "    generator.load_weights(weights_path)\n",
        "    noise = np.random.normal(0, 0.5, (amount, latent_size))\n",
        "    sampled_labels = np.array([\n",
        "        [i] * amount/10 for i in range(10)\n",
        "    ]).reshape(-1, 1)\n",
        "    generated_images = generator.predict([noise, sampled_labels]).transpose(0, 2, 3, 1)\n",
        "    generated_images = np.asarray((generated_images * 127.5 + 127.5).astype(np.uint8))\n",
        "\n",
        "    return generated_images, sampled_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Kj2C4jidyoGe",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9oDmpZvzSiHC",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 2456
        },
        "outputId": "64d62eb9-f762-4bf4-fa63-851b9546178a",
        "executionInfo": {
          "status": "error",
          "timestamp": 1528571232000,
          "user_tz": -180,
          "elapsed": 59002,
          "user": {
            "displayName": "Kateryna Zorina",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "118398898453277774779"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "from keras.datasets import cifar10, mnist\n",
        "import numpy as np\n",
        "#datasets = ['mnist', 'cifar10', 'caltech101']\n",
        "dataset = 'cifar10'\n",
        "num_images = 50000\n",
        "#baseline = build_model(data_augmentation = False, dataset = dataset)\n",
        "#typical_aug = build_model(data_augmentation = True, dataset = dataset)\n",
        "#vae_add_near =\n",
        "#vae_add_random =\n",
        "#import gan_working as gan\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "filepath=\"weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "callbacks_list = [checkpoint]\n",
        "scores = []\n",
        "losses = []\n",
        "\n",
        "#models_to_compare = [gan]#, vae_add_near, vae_add_random, gan]\n",
        "fake_ratio = [0, 0.2, 0.4, 0.6, 0.8, 1]\n",
        "#for model in models_to_compare:\n",
        "for ratio in fake_ratio:\n",
        "    model = build_model(data_augmentation = False, dataset = dataset)\n",
        "    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "    \n",
        "    x_train = x_train[:int(num_images*(1 - ratio))]\n",
        "    fake, sampled_labels = generate_images(num_images*ratio)\n",
        "\n",
        "\n",
        "\n",
        "    x_train = x_train.reshape(x_train.shape[0], 3, 32, 32)\n",
        "    x_test = x_test.reshape(x_test.shape[0], 3, 32, 32)\n",
        "    x_train = x_train.astype('float32')\n",
        "    x_test = x_test.astype('float32')\n",
        "\n",
        "    augmented = np.concatenate((x_train, fake))\n",
        "    print(augmented.shape)\n",
        "    augmented_y = np.concatenate((y_train, sampled_labels))\n",
        "    print(augmented_y.shape)\n",
        "    model.fit(augmented, augmented_y,\n",
        "              batch_size=64,\n",
        "              epochs=2,\n",
        "              verbose=2,\n",
        "              validation_data=(x_test, y_test),\n",
        "              callbacks=callbacks_list)\n",
        "    model.save_weights(\n",
        "        'model-{}-fake.hdf5'.format(ratio), True)\n",
        "    files.download('model-{}-fake.hdf5'.format(ratio))\n",
        "\n",
        "    score = model.evaluate(x_test, y_test, verbose=0)\n",
        "    print('Test loss:', score[0])\n",
        "    scores.append(score[0])\n",
        "    print('Test accuracy:', score[1])\n",
        "    losses.append(score[1])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (50000, 32, 32, 3)\n",
            "50000 train samples\n",
            "10000 test samples\n",
            "(50000, 32, 32, 3)\n",
            "(10000, 32, 32, 3)\n",
            "(50000, 10)\n",
            "(10000, 10)\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_53 (Conv2D)           (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "conv2d_54 (Conv2D)           (None, 30, 30, 32)        9248      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_27 (MaxPooling (None, 15, 15, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_40 (Dropout)         (None, 15, 15, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_55 (Conv2D)           (None, 15, 15, 64)        18496     \n",
            "_________________________________________________________________\n",
            "conv2d_56 (Conv2D)           (None, 13, 13, 64)        36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_28 (MaxPooling (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_41 (Dropout)         (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_15 (Flatten)         (None, 2304)              0         \n",
            "_________________________________________________________________\n",
            "dense_28 (Dense)             (None, 512)               1180160   \n",
            "_________________________________________________________________\n",
            "dropout_42 (Dropout)         (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_29 (Dense)             (None, 10)                5130      \n",
            "=================================================================\n",
            "Total params: 1,250,858\n",
            "Trainable params: 1,250,858\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/2\n",
            " - 28s - loss: 1.4956 - acc: 0.4540 - val_loss: 1.0810 - val_acc: 0.6113\n",
            "Epoch 2/2\n",
            " - 26s - loss: 1.0770 - acc: 0.6192 - val_loss: 0.8994 - val_acc: 0.6848\n",
            "Test loss: 0.8994426242828369\n",
            "Test accuracy: 0.6848\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1566\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1567\u001b[0;31m     \u001b[0mc_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_FinishOperation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1568\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Dimension 3 in both shapes must be equal, but are 4 and 384. Shapes are [5,5,192,4] and [5,5,192,384]. for 'Assign_7' (op: 'Assign') with input shapes: [5,5,192,4], [5,5,192,384].",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-e06ac49eaddb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mx_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_images\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mratio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mfake\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampled_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_images\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mratio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-5377e25e36b5>\u001b[0m in \u001b[0;36mgenerate_images\u001b[0;34m(amount)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgenerate_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mgenerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatent_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mnoise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mamount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     sampled_labels = np.array([\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch, reshape)\u001b[0m\n\u001b[1;32m   2665\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2666\u001b[0m                 load_weights_from_hdf5_group(\n\u001b[0;32m-> 2667\u001b[0;31m                     f, self.layers, reshape=reshape)\n\u001b[0m\u001b[1;32m   2668\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2669\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_updated_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36mload_weights_from_hdf5_group\u001b[0;34m(f, layers, reshape)\u001b[0m\n\u001b[1;32m   3391\u001b[0m                              ' elements.')\n\u001b[1;32m   3392\u001b[0m         \u001b[0mweight_value_tuples\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymbolic_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3393\u001b[0;31m     \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_set_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_value_tuples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mbatch_set_value\u001b[0;34m(tuples)\u001b[0m\n\u001b[1;32m   2370\u001b[0m                 assign_placeholder = tf.placeholder(tf_dtype,\n\u001b[1;32m   2371\u001b[0m                                                     shape=value.shape)\n\u001b[0;32m-> 2372\u001b[0;31m                 \u001b[0massign_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massign_placeholder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2373\u001b[0m                 \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assign_placeholder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0massign_placeholder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2374\u001b[0m                 \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assign_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0massign_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36massign\u001b[0;34m(self, value, use_locking)\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0mthe\u001b[0m \u001b[0massignment\u001b[0m \u001b[0mhas\u001b[0m \u001b[0mcompleted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m     \"\"\"\n\u001b[0;32m--> 615\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mstate_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_locking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_locking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0massign_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_locking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/state_ops.py\u001b[0m in \u001b[0;36massign\u001b[0;34m(ref, value, validate_shape, use_locking, name)\u001b[0m\n\u001b[1;32m    281\u001b[0m     return gen_state_ops.assign(\n\u001b[1;32m    282\u001b[0m         \u001b[0mref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_locking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_locking\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m         validate_shape=validate_shape)\n\u001b[0m\u001b[1;32m    284\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_state_ops.py\u001b[0m in \u001b[0;36massign\u001b[0;34m(ref, value, validate_shape, use_locking, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[1;32m     59\u001b[0m         \u001b[0;34m\"Assign\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         use_locking=use_locking, name=name)\n\u001b[0m\u001b[1;32m     61\u001b[0m     \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    785\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    786\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    788\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_stateful\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[1;32m   3390\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3391\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3392\u001b[0;31m           op_def=op_def)\n\u001b[0m\u001b[1;32m   3393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3394\u001b[0m       \u001b[0;31m# Note: shapes are lazily computed with the C API enabled.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[1;32m   1732\u001b[0m           op_def, inputs, node_def.attr)\n\u001b[1;32m   1733\u001b[0m       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,\n\u001b[0;32m-> 1734\u001b[0;31m                                 control_input_ops)\n\u001b[0m\u001b[1;32m   1735\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1736\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_c_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1568\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1569\u001b[0m     \u001b[0;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1570\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1572\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mc_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Dimension 3 in both shapes must be equal, but are 4 and 384. Shapes are [5,5,192,4] and [5,5,192,384]. for 'Assign_7' (op: 'Assign') with input shapes: [5,5,192,4], [5,5,192,384]."
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "0KZUkRWlSoF5",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}